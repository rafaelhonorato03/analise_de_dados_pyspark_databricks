# Análise de Dados com PySpark e Databricks

Este projeto tem como objetivo realizar análises de dados utilizando PySpark e a plataforma Databricks. Ele foi desenvolvido para demonstrar como manipular, transformar e analisar grandes volumes de dados de forma eficiente, aproveitando as capacidades de processamento distribuído do Apache Spark.

## Estrutura do Projeto

A estrutura principal do projeto é organizada da seguinte forma:

- **`notebooks/`**: Contém notebooks do Databricks com os scripts PySpark para análise de dados.
- **`scripts/`**: Scripts auxiliares para pré-processamento e transformação de dados.
- **`data/`**: Diretório para armazenar os conjuntos de dados utilizados no projeto.
- **`.gitattributes`**: Configuração para normalização de final de linha no Git.
- **`README.md`**: Documentação do projeto.

## Tecnologias Utilizadas

- **PySpark**: Para processamento e análise de dados em larga escala.
- **Databricks**: Plataforma para execução e gerenciamento de pipelines de dados.
- **Python**: Linguagem principal para desenvolvimento.
- **Git**: Controle de versão do código.

## Funcionalidades

1. **Carregamento de Dados**: Importação de dados de diferentes fontes, como arquivos CSV, Parquet ou bancos de dados.
2. **Transformação de Dados**: Limpeza, agregação e transformação dos dados utilizando PySpark.
3. **Análise Exploratória**: Geração de estatísticas descritivas e visualizações para entender os dados.
4. **Integração com Databricks**: Execução de notebooks e pipelines diretamente na plataforma Databricks.

## Como Executar

1. **Configuração do Ambiente**:
   - Certifique-se de ter o Python e o PySpark instalados.
   - Configure sua conta no Databricks e importe os notebooks do projeto.

2. **Execução Local**:
   - Clone o repositório:
     ```bash
     git clone https://github.com/seu-usuario/analise_de_dados_pyspark_databricks.git
     ```
   - Navegue até o diretório do projeto:
     ```bash
     cd analise_de_dados_pyspark_databricks
     ```
   - Execute os scripts ou notebooks localmente.

3. **Execução no Databricks**:
   - Faça upload dos notebooks para o Databricks.
   - Configure os clusters e execute os notebooks.

4. **Acesse o Notebook Publicado**:
   - Você pode visualizar o notebook publicado diretamente no Databricks através do link abaixo:
     [Notebook Publicado no Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/501167806304592/3509913413148908/3731144208576276/latest.html)

## Contribuição

Contribuições são bem-vindas! Sinta-se à vontade para abrir issues ou enviar pull requests com melhorias.

## Licença

Este projeto está licenciado sob a MIT License.

## Contato

Para dúvidas ou sugestões, entre em contato pelo e-mail: `rafael.honorato03@gmail.com`.
